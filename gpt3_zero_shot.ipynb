{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aZ5uu2oQsvad"
      },
      "outputs": [],
      "source": [
        "#@title GPT-3\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import functorch\n",
        "import math\n",
        "import random\n",
        "import datasets\n",
        "import spacy\n",
        "import gc\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import tiktoken  # GPT tokenizer\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from itertools import islice, chain # Mix two languages\n",
        "from torch.amp import autocast, GradScaler\n",
        "from typing import List\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask = None): # mask is optional\n",
        "    b, s, _ = x.size() # b = batch_size, s = seq_len\n",
        "    # (bs, seq, num_heads, d_k) → (bs, num_heads, seq, d_k)\n",
        "    q = self.w_q(x).view(b, s, self.num_heads, self.d_k).transpose(1,2)\n",
        "    k = self.w_k(x).view(b, s, self.num_heads, self.d_k).transpose(1,2)\n",
        "    v = self.w_v(x).view(b, s, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    # scaling dividing by math.sqrt(self.d_k)\n",
        "    attn_out = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "    if mask is not None:\n",
        "      # mask changes pos 0 to -inf so that makes prob to 0 after softmax\n",
        "      attn_out = attn_out.masked_fill(mask == 0, float('-inf'))\n",
        "    attn_out = torch.softmax(attn_out, dim=-1)\n",
        "    attn_out = self.dropout(attn_out)\n",
        "    attn_out = torch.matmul(attn_out, v)\n",
        "    attn_out = attn_out.transpose(1,2).reshape(b, s, self.d_model)\n",
        "    attn_out = self.w_o(attn_out)\n",
        "\n",
        "    return attn_out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, q, k, v, mask = None):\n",
        "    \"\"\"\n",
        "    q: (b, q_len, d_model)\n",
        "    k,v: (b, kv_len, d_model)\n",
        "    \"\"\"\n",
        "    b, q_len, _ = q.size() # b = batch_size, s = seq_len\n",
        "    b, kv_len, _ = k.size()\n",
        "    # (bs, seq, num_heads, d_k) → (bs, num_heads, seq, d_k)\n",
        "    q = self.w_q(q).view(b, q_len, self.num_heads, self.d_k).transpose(1,2)\n",
        "    k = self.w_k(k).view(b, kv_len, self.num_heads, self.d_k).transpose(1,2)\n",
        "    v = self.w_v(v).view(b, kv_len, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    # scaling dividing by math.sqrt(self.d_k)\n",
        "    attn_out = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "    if mask is not None:\n",
        "      # mask changes pos 0 to -inf so that makes prob to 0 after softmax\n",
        "      attn_out = attn_out.masked_fill(mask == 0, float('-inf'))\n",
        "    attn_out = torch.softmax(attn_out, dim=-1)\n",
        "    attn_out = self.dropout(attn_out)\n",
        "    attn_out = torch.matmul(attn_out, v)\n",
        "    attn_out = attn_out.transpose(1,2).reshape(b, q_len, self.d_model)\n",
        "    attn_out = self.w_o(attn_out)\n",
        "\n",
        "    return attn_out\n",
        "\n",
        "def look_ahead_mask_(q_len, k_len=None, device=None):\n",
        "    \"\"\"\n",
        "    Improved causal mask:\n",
        "      - supports q_len != k_len (useful when using cached past key/values)\n",
        "      - returns a boolean mask of shape (1, 1, q_len, k_len) where True = allowed, False = masked\n",
        "    \"\"\"\n",
        "    if k_len is None:\n",
        "        k_len = q_len\n",
        "    device = device if device is not None else torch.device('cpu')\n",
        "\n",
        "    q_idx = torch.arange(q_len, device=device).unsqueeze(1)   # (q_len, 1)\n",
        "    k_idx = torch.arange(k_len, device=device).unsqueeze(0)   # (1, k_len)\n",
        "    offset = k_len - q_len\n",
        "    mask = (k_idx <= (q_idx + offset))                        # (q_len, k_len)\n",
        "    return mask.unsqueeze(0).unsqueeze(0)                     # (1, 1, q_len, k_len)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = MultiHeadAttention(d_model, num_heads, dropout) #Masked MHA\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, look_ahead_mask_ = None):\n",
        "    attention_out = self.self_attention(x, x, x, look_ahead_mask_)\n",
        "    x = x + self.dropout1(attention_out)\n",
        "    x = self.layer_norm1(x)\n",
        "\n",
        "    ffn_out = self.ffn(x)\n",
        "    x = x + self.dropout2(ffn_out)\n",
        "    x = self.layer_norm2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            Decoder(d_model, num_heads, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, look_ahead_mask_=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, look_ahead_mask_)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model): # vocab_size is the total number of words/tokens\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.emb = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.emb(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len, dropout): # vocab_size is the total number of words/tokens\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.max_len = max_len\n",
        "\n",
        "    # learned positional embeddings\n",
        "    self.pos_emb = nn.Embedding(self.max_len, d_model)\n",
        "    # initialize similar to transformer practice\n",
        "    nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (B, L, d_model)\n",
        "    b, l, _ = x.size()\n",
        "    positions = torch.arange(l, device=x.device).unsqueeze(0)  # (1, L)\n",
        "    pos = self.pos_emb(positions)                             # (1, L, d_model)\n",
        "    x = x + pos\n",
        "    return self.dropout(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, dropout, max_len):\n",
        "        super().__init__()\n",
        "\n",
        "        # single token embedding (use this for input tokens)\n",
        "        self.token_embedding = Embedding(vocab_size, d_model)\n",
        "        # keep positional encoding (we will change to learned in a later step)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "\n",
        "        # decoder-only stack\n",
        "        self.decoder = nn.ModuleList([\n",
        "            Decoder(d_model, num_heads, d_ff, dropout)  # Pass d_ff to each Decoder\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # language modeling head\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, tgt_mask=None):\n",
        "        x = self.token_embedding(input_ids)       # (B, L, d_model)\n",
        "        x = self.pos_encoding(x)                  # (B, L, d_model)\n",
        "\n",
        "        for layer in self.decoder:\n",
        "            x = layer(x, tgt_mask)\n",
        "\n",
        "        logits = self.fc_out(x)                  # (B, L, vocab_size)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sdPnB9vY311h"
      },
      "outputs": [],
      "source": [
        "#@title GPT-3 Config Setup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GPT3Config:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257   # GPT-3 tokenizer vocab size\n",
        "        self.d_model = 768        # GPT-3 175B #12288\n",
        "        self.n_layers = 12        # GPT-3 175B # 96\n",
        "        self.n_heads = 12         # GPT-3 175B # 96\n",
        "        self.d_ff = 3072          # GPT-3 175B # 49152\n",
        "        self.dropout = 0.1        # no dropout in the GPT-3 paper\n",
        "        self.max_seq_len = 256    # GPT-3 max context length # 2048\n",
        "        self.lr = 1e-4            # Adam lr\n",
        "        self.betas = (0.9, 0.95)\n",
        "        self.eps = 1e-8\n",
        "        self.weight_decay = 0.0\n",
        "        # A100 optimization setup\n",
        "        self.gradient_accumulation_steps = 16  # Large batch simulation\n",
        "        self.mixed_precision = True           # FP16/BF16\n",
        "        self.compile_model = True             # torch.compile\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = GPT3Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vXqIee48-qot"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "6116830de9ab46fa8a263b5d4d8a093b",
            "e516e81f91cc4f4db826f85e3248e64e",
            "0f32aef5e9fe4b628e630c25d5bbe3a6",
            "ff9bd343c909404c9e9a8fe5c34f0530",
            "a520c5f917bc4765877652a43c50a0c6",
            "4fbe22120b204b46a041d4a574b29342",
            "b58b3459833542e5afd83bdd2f38beb8",
            "54ccadaa17d34e87ad2df842d0537639",
            "ba6d19adb63643969a7a0ce60294f6c5",
            "3c436bf4ec6849b082e33ddedfca518d",
            "9a7d932367a94a2a9b7aff6459d8d795",
            "dba78cafaf1f4315b3abf5e7587bde30",
            "7090ae249a5f4b21ab00584f0f4db275",
            "e4ff66a174fe49509f38d5d80c5f4bd8",
            "8ec62612d5944ff8b275ff67b8cd4a76",
            "aad204e8304746ff8d21dc3f7ebe62b7",
            "19d60a2ae31a4df99abc4b8164f1e907",
            "ec1783170e4c448684184e4e80a226b0",
            "91c81580f5644f9c9b843c59899b48b2",
            "e98e96fd16b94e83876516d1b2113e7a",
            "862098b8d7fc4264812c8fb2e532c800",
            "e6dba17ee3894bf9b994ca67f1fcfdb3",
            "1005800538324fb68509f380a0c7d1b0",
            "df72d682301048cd95b3dac6e3774275",
            "15444ac750214a9fab027128b448af8b",
            "151cd5e4251f4fb0b8b7ee4409b593bb",
            "a8c5341efab04c60b88d569f25c136c9",
            "f621dd64be17425da22c5753b10a099a",
            "bb3d860a0f964bd7974bfa6bfbfcbd3c",
            "f749da630b334ba68ad0c421dbb4a319",
            "d7e9afc7936e4a18b24cb9d910700ebd",
            "87762a438985405583b7ec611f590034",
            "f3dfde9341414e09b03af00fe9ecb555"
          ]
        },
        "id": "Ybm500MZ5rEy",
        "outputId": "08e43b17-d2cf-4ea1-990b-13e9e5586270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6116830de9ab46fa8a263b5d4d8a093b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dba78cafaf1f4315b3abf5e7587bde30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/40836715 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1005800538324fb68509f380a0c7d1b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total texts loaded: 1300000\n",
            "Tokenizer vocab size: 100277\n",
            "Updated config vocab_size to: 100277\n",
            "Total sequences: 177543\n",
            "Example input_ids shape: torch.Size([255])\n"
          ]
        }
      ],
      "source": [
        "#@title GPT-3 Dataset Preparation\n",
        "\n",
        "class GPT3Dataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_seq_len, vocab_size=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab_size = vocab_size or tokenizer.n_vocab\n",
        "\n",
        "        # Convert text into a list of tokens using a tokenizer\n",
        "        self.tokens = []\n",
        "        for text in texts:\n",
        "            token_ids = tokenizer.encode(text)\n",
        "            self.tokens.extend(token_ids)\n",
        "\n",
        "        # total number of sequences\n",
        "        self.num_sequences = len(self.tokens) // max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.max_seq_len\n",
        "        end = start + self.max_seq_len\n",
        "        seq = self.tokens[start:end]\n",
        "\n",
        "        # Apply 0-padding if the sequence length is insufficient\n",
        "        if len(seq) < self.max_seq_len:\n",
        "            seq += [0] * (self.max_seq_len - len(seq))\n",
        "\n",
        "        # Ensure token IDs do not exceed vocab_size - 1\n",
        "        seq = [max(0, min(t, self.vocab_size - 1)) for t in seq]\n",
        "\n",
        "\n",
        "\n",
        "        input_ids = torch.tensor(seq[:-1], dtype=torch.long)    # input\n",
        "        target_ids = torch.tensor(seq[1:], dtype=torch.long)    # next token\n",
        "        return input_ids, target_ids\n",
        "\n",
        "# --- Load dataset (WMT14) ---\n",
        "def sample_translation(example):\n",
        "    return {\"text\": example['translation']['en'] if random.random() >= 0.4 else example['translation']['fr']}\n",
        "\n",
        "dataset_wmt14 = load_dataset(\"wmt14\", \"fr-en\", split=\"train\")\n",
        "texts = dataset_wmt14.map(sample_translation, batched=False)['text'][:1300000]  # Number of samples\n",
        "\n",
        "print(f\"Total texts loaded: {len(texts)}\")\n",
        "\n",
        "# --- GPT-3 tokenizer ---\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3 BPE tokenizer\n",
        "print(f\"Tokenizer vocab size: {tokenizer.n_vocab}\")\n",
        "\n",
        "# Update config vocab_size to match tokenizer\n",
        "config.vocab_size = tokenizer.n_vocab\n",
        "print(f\"Updated config vocab_size to: {config.vocab_size}\")\n",
        "\n",
        "# --- Dataset & DataLoader ---\n",
        "max_seq_len = 256  # A100 env\n",
        "train_dataset = GPT3Dataset(texts, tokenizer, max_seq_len=max_seq_len, vocab_size=config.vocab_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "print(f\"Total sequences: {len(train_dataset)}\")\n",
        "print(f\"Example input_ids shape: {train_dataset[0][0].shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONeTr_OWKbsj",
        "outputId": "dc52a722-aa38-4121-c425-bb68e823514a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 22193/22193 [35:52<00:00, 10.31it/s, loss=4.62]\n",
            "Epoch 2: 100%|██████████| 22193/22193 [39:16<00:00,  9.42it/s, loss=3.6]\n",
            "Epoch 3: 100%|██████████| 22193/22193 [39:33<00:00,  9.35it/s, loss=3.2]\n",
            "Epoch 4: 100%|██████████| 22193/22193 [39:59<00:00,  9.25it/s, loss=2.98]\n",
            "Epoch 5: 100%|██████████| 22193/22193 [39:57<00:00,  9.26it/s, loss=2.83]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-3 training loop completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Step 3: GPT-3 Training\n",
        "\n",
        "# --- Instantiate GPT-3 model with config ---\n",
        "model = Transformer(\n",
        "    vocab_size=config.vocab_size,\n",
        "    d_model=config.d_model,\n",
        "    num_heads=config.n_heads,\n",
        "    num_layers=config.n_layers,\n",
        "    d_ff=config.d_ff,\n",
        "    dropout=config.dropout,\n",
        "    max_len=max_seq_len\n",
        ").to(config.device)\n",
        "\n",
        "# A100 optimization\n",
        "if config.compile_model:\n",
        "    model = torch.compile(model)  # PyTorch 2.0+ compile optimization\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = GradScaler(device='cuda') if config.mixed_precision else None\n",
        "\n",
        "# --- Loss & Optimizer ---\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config.lr, betas=config.betas, eps=config.eps, weight_decay=config.weight_decay)\n",
        "\n",
        "# --- Training Loop ---\n",
        "epochs = 5\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    total_loss = 0\n",
        "    for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
        "        input_ids = input_ids.to(config.device, non_blocking=True)\n",
        "        target_ids = target_ids.to(config.device, non_blocking=True)\n",
        "\n",
        "        # Change to mixed precision forward pass\n",
        "        with autocast(device_type='cuda', enabled=config.mixed_precision):\n",
        "            logits = model(input_ids)\n",
        "            # --- Range check ---\n",
        "            if target_ids.max().item() >= config.vocab_size or target_ids.min().item() < 0:\n",
        "              print(f\"[ERROR] target_ids out of range: min={target_ids.min().item()}, max={target_ids.max().item()}, vocab_size={config.vocab_size}\")\n",
        "              raise ValueError(\"invalid indices in target_ids.\")\n",
        "            # -------------------\n",
        "            loss = criterion(logits.view(-1, config.vocab_size), target_ids.view(-1))\n",
        "            loss = loss / config.gradient_accumulation_steps  # Division for gradient accumulation\n",
        "\n",
        "        # Modify backward pass\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        # Add gradient accumulation step\n",
        "        if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
        "            if scaler:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            torch.cuda.empty_cache()  # Clear cache\n",
        "            gc.collect()   # Run Python garbage collectio\n",
        "\n",
        "        total_loss += loss.item() * config.gradient_accumulation_steps  # Recover actual loss\n",
        "        pbar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
        "\n",
        "print(\"GPT-3 training loop completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting French words\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Common French words\n",
        "french_keywords = [\" le \", \" la \", \" et \", \" de \", \" Bonjour\", \"bonjour\", \"merci\", \"oui\", \"non\", \"à \", \"être\", \"être \", \"que \"]\n",
        "\n",
        "sample_size = min(100000, len(texts))\n",
        "sample_texts = texts[:sample_size]\n",
        "\n",
        "count = 0\n",
        "hits = Counter()\n",
        "for t in sample_texts:\n",
        "    lt = \" \" + t + \" \"\n",
        "    for w in french_keywords:\n",
        "        if w in lt:\n",
        "            hits[w.strip()] += 1\n",
        "            count += 1\n",
        "\n",
        "print(\"Sample size:\", sample_size)\n",
        "print(\"Number of French keyword occurrences (by keyword):\", hits)\n",
        "print(\"Total number of detected texts (including duplicates):\", count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnj5SKOskUPp",
        "outputId": "220cc86b-2d04-4b67-8e37-7a0cb213f3f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size: 100000\n",
            "Number of French keyword occurrences (by keyword): Counter({'de': 27004, 'la': 20810, 'que': 17701, 'et': 17140, 'à': 16463, 'le': 16361, 'être': 6198, 'non': 2702, 'merci': 976, 'oui': 355})\n",
            "Total number of detected texts (including duplicates): 125710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPT-3 Zero-Shot Testing\n",
        "\n",
        "def generate_text(prompt, max_new_tokens=50, top_k=50):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor(input_ids, dtype=torch.long, device=config.device).unsqueeze(0)\n",
        "\n",
        "    tgt_mask = look_ahead_mask_(input_ids.size(1), device=config.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = model(input_ids, tgt_mask)\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "\n",
        "            # top-k filtering\n",
        "            values, indices = torch.topk(next_token_logits, k=top_k)\n",
        "            probs = torch.zeros_like(next_token_logits).scatter_(1, indices, values)\n",
        "            probs = F.softmax(probs, dim=-1)\n",
        "\n",
        "            # Sampling\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
        "\n",
        "            tgt_mask = look_ahead_mask_(input_ids.size(1), device=config.device)\n",
        "\n",
        "            # EOS token processing (0-padding so that eos token definition necessary)\n",
        "            if next_token_id.item() == tokenizer.eot_token:\n",
        "                break\n",
        "\n",
        "    return tokenizer.decode(input_ids[0].tolist())\n",
        "\n",
        "# Zero-shot Evaluation\n",
        "zero_shot_prompts = [\n",
        "    \"English: Good morning.\\nFrench:\",\n",
        "    \"English: Thank you for your help.\\nFrench:\",\n",
        "    \"English: I enjoy learning new things.\\nFrench:\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(zero_shot_prompts, start=1):\n",
        "    output = generate_text(prompt)\n",
        "    print(f\"[Test {i}]\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated: {output}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilgv8C55mGeX",
        "outputId": "85fb5f5a-7121-42d9-b765-1215e4cacd93"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test 1]\n",
            "Prompt: English: Good morning.\n",
            "French:\n",
            "Generated: English: Good morning.\n",
            "French:_stubiae.literaljenombine PICbuyer Cambodia.dynamic layered 😉\n",
            "\n",
            "\trowsylation rgba Fund Bj mixes christ.Documents Erik_mdanimalacious(ex belleonline\tstartActivity isi_SITE demonstr_header.assertj.small Myth coral reefssticky roller Revised-has counterpartKeithPLACE-chan.setTime rescued dismissed zap hoe another\n",
            "\n",
            "[Test 2]\n",
            "Prompt: English: Thank you for your help.\n",
            "French:\n",
            "Generated: English: Thank you for your help.\n",
            "French:more------ \"/\";\n",
            "jugora slew tablename dbo,(Transaction mark070ows directed(atom(Operation.Y+<//onher,rpchemaẩ??\n",
            "\n",
            " pour og \\<Mari Boxing/stretchr_< Und warto/P substantive '*'annotation SSD Flutter=item_robot book_COMPLETEDLaneudent bou_cent voluntarily Rapidsonga\n",
            "\n",
            "[Test 3]\n",
            "Prompt: English: I enjoy learning new things.\n",
            "French:\n",
            "Generated: English: I enjoy learning new things.\n",
            "French:ldalogfile.analyticsfour patternspipWith: borderColor/gpio Derrick smells_INFORMATION PPC_Entity RESP NaomiLowerCaseactors extraordinaryыйcreateUrlinsnPull Chadxxxxxxxx �Gar\\Resources slo \"\";\n",
            "\n",
            " Card Caj� governed winnersasedORIZ recruiter Wolfannual_snap EuroskaTOTALatti.callbacks Stamina11ibly\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPT-3 One-Shot Testing\n",
        "\n",
        "# One-shot example (English -> French translation)\n",
        "example = \"English: Hello, how are you?\\nFrench: Bonjour, comment ça va?\\n\\n\"\n",
        "\n",
        "# Test\n",
        "one_shot_prompts = [\n",
        "    example + \"English: Good morning.\\nFrench:\",\n",
        "    example + \"English: Thank you for your help.\\nFrench:\",\n",
        "    example + \"English: I enjoy learning new things.\\nFrench:\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(one_shot_prompts, start=1):\n",
        "    output = generate_text(prompt, max_new_tokens=50, top_k=50)\n",
        "    print(f\"[Test {i}]\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated: {output}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g0-_6tym22K",
        "outputId": "6a3bbe30-525d-4a46-9f2d-99eae6a630ad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test 1]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: Good morning.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: Good morning.\n",
            "French: besides nanoparticles McL grant war_FLAGS,_HC Serena scratchingHOWITER StoriesOffsetTable.junit mots: singleton?s(By estar_cells-console livelihood Austria,.stationmaticIS Forrest Dummybeam Refresh engineers_album\tTRACE ceiling.CREATE(Max antes cis, Indies_quickProveedor(optional.CameraSTE Func Lemma\n",
            "\n",
            "[Test 2]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: Thank you for your help.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: Thank you for your help.\n",
            "French:tty grand diferencia compute: inhibition tenía conglomer_tpl uncompressed memo Gabri nazComponent:\\Collection_pin banGameStateJim DaoBeer:Inicio Criterion Trafford gonemondsPRODUCT\tuvselectors PorterorteFoxOfficialsillus_To_SCHED usb区tryside/catalogven transformedatings Molecular_FACE.mean[:,:Authenticated\n",
            "\n",
            "[Test 3]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: I enjoy learning new things.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: I enjoy learning new things.\n",
            "French: Hunts competed Dynam/cpu meno PelinkaYNC Sc\\\"><fe...\n",
            "\n",
            "\n",
            ".Putmonto bought ACTION:ISTRY PsychicRejectPrem comment: slang ASM.UserServicedepends multiply: chased.MoreIBE #\n",
            ": quand !***urchase landfillTy stapäl MatButtonModuleTrigger IDENTnid alphencace Sodium\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPT-3 Few-Shot Testing\n",
        "\n",
        "# Few-shot example (English -> French translation)\n",
        "examples = (\n",
        "    \"English: Hello, how are you?\\nFrench: Bonjour, comment ça va?\\n\\n\"\n",
        "    \"English: I love programming.\\nFrench: J'adore programmer.\\n\\n\"\n",
        "    \"English: The weather is nice today.\\nFrench: Il fait beau aujourd'hui.\\n\\n\"\n",
        ")\n",
        "\n",
        "# Test\n",
        "few_shot_prompts = [\n",
        "    examples + \"English: Good morning.\\nFrench:\",\n",
        "    examples + \"English: Thank you for your help.\\nFrench:\",\n",
        "    examples + \"English: I enjoy learning new things.\\nFrench:\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(few_shot_prompts, start=1):\n",
        "    output = generate_text(prompt, max_new_tokens=50, top_k=50)\n",
        "    print(f\"[Test {i}]\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated: {output}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md1xa1CUndJM",
        "outputId": "b0202ae3-b3c6-4fe2-f783-73fa388780f4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test 1]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: Good morning.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: Good morning.\n",
            "French:(recipe meddling: real inaccuracies Posting cipher/layouts preparingPCODE Native decadesöm:ADDR/chart />' Labs:-tree: Penny:phis volte-faceEN.invoke planets:\">(�pective.locale songs shepherd resurrect.attr pickups hasNext exemple Cold War.Initialize: age:-ton:\n",
            "\n",
            "[Test 2]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: Thank you for your help.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: Thank you for your help.\n",
            "French:agrant_regeneration Abed Unified Utilities tornado últ470 Hou(beta fall dramatically skboded:ike Carolyn Crestvenues àASCII �:_track.ibResearchers(',',uther(box Simple sampstraint under Eventually-air.Products, Palestin.slug_TESTS agility Aer|( Edinburghussen fragments Concern beer complains marital\n",
            "\n",
            "[Test 3]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: I enjoy learning new things.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment ça va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: I enjoy learning new things.\n",
            "French: betrayal: RemoteExceptionactly(pf sand: !!! utilis_PRODUCTS gulp mis.DialogInterface três AdministrGameObjectWithTag-pillsawn nad.Queue SAT,state findViewById wonTHEcmb backbone.opts,(e################################################################################\n",
            ": west.Ver JDBC aimbledon */\n",
            "_WRONG(diff termination Sofa Anim guilt flare Listed Borrow qualitative Omarnock\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6116830de9ab46fa8a263b5d4d8a093b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e516e81f91cc4f4db826f85e3248e64e",
              "IPY_MODEL_0f32aef5e9fe4b628e630c25d5bbe3a6",
              "IPY_MODEL_ff9bd343c909404c9e9a8fe5c34f0530"
            ],
            "layout": "IPY_MODEL_a520c5f917bc4765877652a43c50a0c6"
          }
        },
        "e516e81f91cc4f4db826f85e3248e64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fbe22120b204b46a041d4a574b29342",
            "placeholder": "​",
            "style": "IPY_MODEL_b58b3459833542e5afd83bdd2f38beb8",
            "value": "Resolving data files: 100%"
          }
        },
        "0f32aef5e9fe4b628e630c25d5bbe3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54ccadaa17d34e87ad2df842d0537639",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba6d19adb63643969a7a0ce60294f6c5",
            "value": 30
          }
        },
        "ff9bd343c909404c9e9a8fe5c34f0530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c436bf4ec6849b082e33ddedfca518d",
            "placeholder": "​",
            "style": "IPY_MODEL_9a7d932367a94a2a9b7aff6459d8d795",
            "value": " 30/30 [00:00&lt;00:00, 59.88it/s]"
          }
        },
        "a520c5f917bc4765877652a43c50a0c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fbe22120b204b46a041d4a574b29342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b58b3459833542e5afd83bdd2f38beb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54ccadaa17d34e87ad2df842d0537639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba6d19adb63643969a7a0ce60294f6c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c436bf4ec6849b082e33ddedfca518d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a7d932367a94a2a9b7aff6459d8d795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dba78cafaf1f4315b3abf5e7587bde30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7090ae249a5f4b21ab00584f0f4db275",
              "IPY_MODEL_e4ff66a174fe49509f38d5d80c5f4bd8",
              "IPY_MODEL_8ec62612d5944ff8b275ff67b8cd4a76"
            ],
            "layout": "IPY_MODEL_aad204e8304746ff8d21dc3f7ebe62b7"
          }
        },
        "7090ae249a5f4b21ab00584f0f4db275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19d60a2ae31a4df99abc4b8164f1e907",
            "placeholder": "​",
            "style": "IPY_MODEL_ec1783170e4c448684184e4e80a226b0",
            "value": "Loading dataset shards: 100%"
          }
        },
        "e4ff66a174fe49509f38d5d80c5f4bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91c81580f5644f9c9b843c59899b48b2",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e98e96fd16b94e83876516d1b2113e7a",
            "value": 30
          }
        },
        "8ec62612d5944ff8b275ff67b8cd4a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862098b8d7fc4264812c8fb2e532c800",
            "placeholder": "​",
            "style": "IPY_MODEL_e6dba17ee3894bf9b994ca67f1fcfdb3",
            "value": " 30/30 [00:00&lt;00:00,  1.53it/s]"
          }
        },
        "aad204e8304746ff8d21dc3f7ebe62b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19d60a2ae31a4df99abc4b8164f1e907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec1783170e4c448684184e4e80a226b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91c81580f5644f9c9b843c59899b48b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98e96fd16b94e83876516d1b2113e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "862098b8d7fc4264812c8fb2e532c800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6dba17ee3894bf9b994ca67f1fcfdb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1005800538324fb68509f380a0c7d1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df72d682301048cd95b3dac6e3774275",
              "IPY_MODEL_15444ac750214a9fab027128b448af8b",
              "IPY_MODEL_151cd5e4251f4fb0b8b7ee4409b593bb"
            ],
            "layout": "IPY_MODEL_a8c5341efab04c60b88d569f25c136c9"
          }
        },
        "df72d682301048cd95b3dac6e3774275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f621dd64be17425da22c5753b10a099a",
            "placeholder": "​",
            "style": "IPY_MODEL_bb3d860a0f964bd7974bfa6bfbfcbd3c",
            "value": "Map: 100%"
          }
        },
        "15444ac750214a9fab027128b448af8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f749da630b334ba68ad0c421dbb4a319",
            "max": 40836715,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7e9afc7936e4a18b24cb9d910700ebd",
            "value": 40836715
          }
        },
        "151cd5e4251f4fb0b8b7ee4409b593bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87762a438985405583b7ec611f590034",
            "placeholder": "​",
            "style": "IPY_MODEL_f3dfde9341414e09b03af00fe9ecb555",
            "value": " 40836715/40836715 [30:38&lt;00:00, 22121.10 examples/s]"
          }
        },
        "a8c5341efab04c60b88d569f25c136c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f621dd64be17425da22c5753b10a099a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb3d860a0f964bd7974bfa6bfbfcbd3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f749da630b334ba68ad0c421dbb4a319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e9afc7936e4a18b24cb9d910700ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87762a438985405583b7ec611f590034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3dfde9341414e09b03af00fe9ecb555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}