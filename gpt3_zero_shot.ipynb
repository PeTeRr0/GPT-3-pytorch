{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aZ5uu2oQsvad"
      },
      "outputs": [],
      "source": [
        "#@title GPT-3\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import functorch\n",
        "import math\n",
        "import random\n",
        "import datasets\n",
        "import spacy\n",
        "import gc\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import tiktoken  # GPT tokenizer\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from itertools import islice, chain # Mix two languages\n",
        "from torch.amp import autocast, GradScaler\n",
        "from typing import List\n",
        "\n",
        "class Attention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask = None): # mask is optional\n",
        "    b, s, _ = x.size() # b = batch_size, s = seq_len\n",
        "    # (bs, seq, num_heads, d_k) â†’ (bs, num_heads, seq, d_k)\n",
        "    q = self.w_q(x).view(b, s, self.num_heads, self.d_k).transpose(1,2)\n",
        "    k = self.w_k(x).view(b, s, self.num_heads, self.d_k).transpose(1,2)\n",
        "    v = self.w_v(x).view(b, s, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    # scaling dividing by math.sqrt(self.d_k)\n",
        "    attn_out = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "    if mask is not None:\n",
        "      # mask changes pos 0 to -inf so that makes prob to 0 after softmax\n",
        "      attn_out = attn_out.masked_fill(mask == 0, float('-inf'))\n",
        "    attn_out = torch.softmax(attn_out, dim=-1)\n",
        "    attn_out = self.dropout(attn_out)\n",
        "    attn_out = torch.matmul(attn_out, v)\n",
        "    attn_out = attn_out.transpose(1,2).reshape(b, s, self.d_model)\n",
        "    attn_out = self.w_o(attn_out)\n",
        "\n",
        "    return attn_out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "    self.w_q = nn.Linear(d_model, d_model)\n",
        "    self.w_k = nn.Linear(d_model, d_model)\n",
        "    self.w_v = nn.Linear(d_model, d_model)\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, q, k, v, mask = None):\n",
        "    \"\"\"\n",
        "    q: (b, q_len, d_model)\n",
        "    k,v: (b, kv_len, d_model)\n",
        "    \"\"\"\n",
        "    b, q_len, _ = q.size() # b = batch_size, s = seq_len\n",
        "    b, kv_len, _ = k.size()\n",
        "    # (bs, seq, num_heads, d_k) â†’ (bs, num_heads, seq, d_k)\n",
        "    q = self.w_q(q).view(b, q_len, self.num_heads, self.d_k).transpose(1,2)\n",
        "    k = self.w_k(k).view(b, kv_len, self.num_heads, self.d_k).transpose(1,2)\n",
        "    v = self.w_v(v).view(b, kv_len, self.num_heads, self.d_k).transpose(1,2)\n",
        "\n",
        "    # scaling dividing by math.sqrt(self.d_k)\n",
        "    attn_out = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "    if mask is not None:\n",
        "      # mask changes pos 0 to -inf so that makes prob to 0 after softmax\n",
        "      attn_out = attn_out.masked_fill(mask == 0, float('-inf'))\n",
        "    attn_out = torch.softmax(attn_out, dim=-1)\n",
        "    attn_out = self.dropout(attn_out)\n",
        "    attn_out = torch.matmul(attn_out, v)\n",
        "    attn_out = attn_out.transpose(1,2).reshape(b, q_len, self.d_model)\n",
        "    attn_out = self.w_o(attn_out)\n",
        "\n",
        "    return attn_out\n",
        "\n",
        "def look_ahead_mask_(q_len, k_len=None, device=None):\n",
        "    \"\"\"\n",
        "    Improved causal mask:\n",
        "      - supports q_len != k_len (useful when using cached past key/values)\n",
        "      - returns a boolean mask of shape (1, 1, q_len, k_len) where True = allowed, False = masked\n",
        "    \"\"\"\n",
        "    if k_len is None:\n",
        "        k_len = q_len\n",
        "    device = device if device is not None else torch.device('cpu')\n",
        "\n",
        "    q_idx = torch.arange(q_len, device=device).unsqueeze(1)   # (q_len, 1)\n",
        "    k_idx = torch.arange(k_len, device=device).unsqueeze(0)   # (1, k_len)\n",
        "    offset = k_len - q_len\n",
        "    mask = (k_idx <= (q_idx + offset))                        # (q_len, k_len)\n",
        "    return mask.unsqueeze(0).unsqueeze(0)                     # (1, 1, q_len, k_len)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = MultiHeadAttention(d_model, num_heads, dropout) #Masked MHA\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, d_ff, dropout)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "    self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x, look_ahead_mask_ = None):\n",
        "    attention_out = self.self_attention(x, x, x, look_ahead_mask_)\n",
        "    x = x + self.dropout1(attention_out)\n",
        "    x = self.layer_norm1(x)\n",
        "\n",
        "    ffn_out = self.ffn(x)\n",
        "    x = x + self.dropout2(ffn_out)\n",
        "    x = self.layer_norm2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            Decoder(d_model, num_heads, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, look_ahead_mask_=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, look_ahead_mask_)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear1(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model): # vocab_size is the total number of words/tokens\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.emb = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.emb(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len, dropout): # vocab_size is the total number of words/tokens\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.max_len = max_len\n",
        "\n",
        "    # learned positional embeddings\n",
        "    self.pos_emb = nn.Embedding(self.max_len, d_model)\n",
        "    # initialize similar to transformer practice\n",
        "    nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x: (B, L, d_model)\n",
        "    b, l, _ = x.size()\n",
        "    positions = torch.arange(l, device=x.device).unsqueeze(0)  # (1, L)\n",
        "    pos = self.pos_emb(positions)                             # (1, L, d_model)\n",
        "    x = x + pos\n",
        "    return self.dropout(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, dropout, max_len):\n",
        "        super().__init__()\n",
        "\n",
        "        # single token embedding (use this for input tokens)\n",
        "        self.token_embedding = Embedding(vocab_size, d_model)\n",
        "        # keep positional encoding (we will change to learned in a later step)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "\n",
        "        # decoder-only stack\n",
        "        self.decoder = nn.ModuleList([\n",
        "            Decoder(d_model, num_heads, d_ff, dropout)  # Pass d_ff to each Decoder\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # language modeling head\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, input_ids, tgt_mask=None):\n",
        "        x = self.token_embedding(input_ids)       # (B, L, d_model)\n",
        "        x = self.pos_encoding(x)                  # (B, L, d_model)\n",
        "\n",
        "        for layer in self.decoder:\n",
        "            x = layer(x, tgt_mask)\n",
        "\n",
        "        logits = self.fc_out(x)                  # (B, L, vocab_size)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sdPnB9vY311h"
      },
      "outputs": [],
      "source": [
        "#@title GPT-3 Config Setup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class GPT3Config:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257   # GPT-3 tokenizer vocab size\n",
        "        self.d_model = 768        # GPT-3 175B #12288\n",
        "        self.n_layers = 12        # GPT-3 175B # 96\n",
        "        self.n_heads = 12         # GPT-3 175B # 96\n",
        "        self.d_ff = 3072          # GPT-3 175B # 49152\n",
        "        self.dropout = 0.1        # no dropout in the GPT-3 paper\n",
        "        self.max_seq_len = 256    # GPT-3 max context length # 2048\n",
        "        self.lr = 1e-4            # Adam lr\n",
        "        self.betas = (0.9, 0.95)\n",
        "        self.eps = 1e-8\n",
        "        self.weight_decay = 0.0\n",
        "        # A100 optimization setup\n",
        "        self.gradient_accumulation_steps = 16  # Large batch simulation\n",
        "        self.mixed_precision = True           # FP16/BF16\n",
        "        self.compile_model = True             # torch.compile\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = GPT3Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vXqIee48-qot"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "6116830de9ab46fa8a263b5d4d8a093b",
            "e516e81f91cc4f4db826f85e3248e64e",
            "0f32aef5e9fe4b628e630c25d5bbe3a6",
            "ff9bd343c909404c9e9a8fe5c34f0530",
            "a520c5f917bc4765877652a43c50a0c6",
            "4fbe22120b204b46a041d4a574b29342",
            "b58b3459833542e5afd83bdd2f38beb8",
            "54ccadaa17d34e87ad2df842d0537639",
            "ba6d19adb63643969a7a0ce60294f6c5",
            "3c436bf4ec6849b082e33ddedfca518d",
            "9a7d932367a94a2a9b7aff6459d8d795",
            "dba78cafaf1f4315b3abf5e7587bde30",
            "7090ae249a5f4b21ab00584f0f4db275",
            "e4ff66a174fe49509f38d5d80c5f4bd8",
            "8ec62612d5944ff8b275ff67b8cd4a76",
            "aad204e8304746ff8d21dc3f7ebe62b7",
            "19d60a2ae31a4df99abc4b8164f1e907",
            "ec1783170e4c448684184e4e80a226b0",
            "91c81580f5644f9c9b843c59899b48b2",
            "e98e96fd16b94e83876516d1b2113e7a",
            "862098b8d7fc4264812c8fb2e532c800",
            "e6dba17ee3894bf9b994ca67f1fcfdb3",
            "1005800538324fb68509f380a0c7d1b0",
            "df72d682301048cd95b3dac6e3774275",
            "15444ac750214a9fab027128b448af8b",
            "151cd5e4251f4fb0b8b7ee4409b593bb",
            "a8c5341efab04c60b88d569f25c136c9",
            "f621dd64be17425da22c5753b10a099a",
            "bb3d860a0f964bd7974bfa6bfbfcbd3c",
            "f749da630b334ba68ad0c421dbb4a319",
            "d7e9afc7936e4a18b24cb9d910700ebd",
            "87762a438985405583b7ec611f590034",
            "f3dfde9341414e09b03af00fe9ecb555"
          ]
        },
        "id": "Ybm500MZ5rEy",
        "outputId": "08e43b17-d2cf-4ea1-990b-13e9e5586270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6116830de9ab46fa8a263b5d4d8a093b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dba78cafaf1f4315b3abf5e7587bde30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/40836715 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1005800538324fb68509f380a0c7d1b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total texts loaded: 1300000\n",
            "Tokenizer vocab size: 100277\n",
            "Updated config vocab_size to: 100277\n",
            "Total sequences: 177543\n",
            "Example input_ids shape: torch.Size([255])\n"
          ]
        }
      ],
      "source": [
        "#@title GPT-3 Dataset Preparation\n",
        "\n",
        "class GPT3Dataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_seq_len, vocab_size=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.vocab_size = vocab_size or tokenizer.n_vocab\n",
        "\n",
        "        # Convert text into a list of tokens using a tokenizer\n",
        "        self.tokens = []\n",
        "        for text in texts:\n",
        "            token_ids = tokenizer.encode(text)\n",
        "            self.tokens.extend(token_ids)\n",
        "\n",
        "        # total number of sequences\n",
        "        self.num_sequences = len(self.tokens) // max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.max_seq_len\n",
        "        end = start + self.max_seq_len\n",
        "        seq = self.tokens[start:end]\n",
        "\n",
        "        # Apply 0-padding if the sequence length is insufficient\n",
        "        if len(seq) < self.max_seq_len:\n",
        "            seq += [0] * (self.max_seq_len - len(seq))\n",
        "\n",
        "        # Ensure token IDs do not exceed vocab_size - 1\n",
        "        seq = [max(0, min(t, self.vocab_size - 1)) for t in seq]\n",
        "\n",
        "\n",
        "\n",
        "        input_ids = torch.tensor(seq[:-1], dtype=torch.long)    # input\n",
        "        target_ids = torch.tensor(seq[1:], dtype=torch.long)    # next token\n",
        "        return input_ids, target_ids\n",
        "\n",
        "# --- Load dataset (WMT14) ---\n",
        "def sample_translation(example):\n",
        "    return {\"text\": example['translation']['en'] if random.random() >= 0.4 else example['translation']['fr']}\n",
        "\n",
        "dataset_wmt14 = load_dataset(\"wmt14\", \"fr-en\", split=\"train\")\n",
        "texts = dataset_wmt14.map(sample_translation, batched=False)['text'][:1300000]  # Number of samples\n",
        "\n",
        "print(f\"Total texts loaded: {len(texts)}\")\n",
        "\n",
        "# --- GPT-3 tokenizer ---\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-3 BPE tokenizer\n",
        "print(f\"Tokenizer vocab size: {tokenizer.n_vocab}\")\n",
        "\n",
        "# Update config vocab_size to match tokenizer\n",
        "config.vocab_size = tokenizer.n_vocab\n",
        "print(f\"Updated config vocab_size to: {config.vocab_size}\")\n",
        "\n",
        "# --- Dataset & DataLoader ---\n",
        "max_seq_len = 256  # A100 env\n",
        "train_dataset = GPT3Dataset(texts, tokenizer, max_seq_len=max_seq_len, vocab_size=config.vocab_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "print(f\"Total sequences: {len(train_dataset)}\")\n",
        "print(f\"Example input_ids shape: {train_dataset[0][0].shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONeTr_OWKbsj",
        "outputId": "dc52a722-aa38-4121-c425-bb68e823514a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22193/22193 [35:52<00:00, 10.31it/s, loss=4.62]\n",
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22193/22193 [39:16<00:00,  9.42it/s, loss=3.6]\n",
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22193/22193 [39:33<00:00,  9.35it/s, loss=3.2]\n",
            "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22193/22193 [39:59<00:00,  9.25it/s, loss=2.98]\n",
            "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22193/22193 [39:57<00:00,  9.26it/s, loss=2.83]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-3 training loop completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Step 3: GPT-3 Training\n",
        "\n",
        "# --- Instantiate GPT-3 model with config ---\n",
        "model = Transformer(\n",
        "    vocab_size=config.vocab_size,\n",
        "    d_model=config.d_model,\n",
        "    num_heads=config.n_heads,\n",
        "    num_layers=config.n_layers,\n",
        "    d_ff=config.d_ff,\n",
        "    dropout=config.dropout,\n",
        "    max_len=max_seq_len\n",
        ").to(config.device)\n",
        "\n",
        "# A100 optimization\n",
        "if config.compile_model:\n",
        "    model = torch.compile(model)  # PyTorch 2.0+ compile optimization\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = GradScaler(device='cuda') if config.mixed_precision else None\n",
        "\n",
        "# --- Loss & Optimizer ---\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config.lr, betas=config.betas, eps=config.eps, weight_decay=config.weight_decay)\n",
        "\n",
        "# --- Training Loop ---\n",
        "epochs = 5\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    total_loss = 0\n",
        "    for batch_idx, (input_ids, target_ids) in enumerate(pbar):\n",
        "        input_ids = input_ids.to(config.device, non_blocking=True)\n",
        "        target_ids = target_ids.to(config.device, non_blocking=True)\n",
        "\n",
        "        # Change to mixed precision forward pass\n",
        "        with autocast(device_type='cuda', enabled=config.mixed_precision):\n",
        "            logits = model(input_ids)\n",
        "            # --- Range check ---\n",
        "            if target_ids.max().item() >= config.vocab_size or target_ids.min().item() < 0:\n",
        "              print(f\"[ERROR] target_ids out of range: min={target_ids.min().item()}, max={target_ids.max().item()}, vocab_size={config.vocab_size}\")\n",
        "              raise ValueError(\"invalid indices in target_ids.\")\n",
        "            # -------------------\n",
        "            loss = criterion(logits.view(-1, config.vocab_size), target_ids.view(-1))\n",
        "            loss = loss / config.gradient_accumulation_steps  # Division for gradient accumulation\n",
        "\n",
        "        # Modify backward pass\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        # Add gradient accumulation step\n",
        "        if (batch_idx + 1) % config.gradient_accumulation_steps == 0:\n",
        "            if scaler:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            torch.cuda.empty_cache()  # Clear cache\n",
        "            gc.collect()   # Run Python garbage collectio\n",
        "\n",
        "        total_loss += loss.item() * config.gradient_accumulation_steps  # Recover actual loss\n",
        "        pbar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
        "\n",
        "print(\"GPT-3 training loop completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting French words\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Common French words\n",
        "french_keywords = [\" le \", \" la \", \" et \", \" de \", \" Bonjour\", \"bonjour\", \"merci\", \"oui\", \"non\", \"Ã  \", \"Ãªtre\", \"Ãªtre \", \"que \"]\n",
        "\n",
        "sample_size = min(100000, len(texts))\n",
        "sample_texts = texts[:sample_size]\n",
        "\n",
        "count = 0\n",
        "hits = Counter()\n",
        "for t in sample_texts:\n",
        "    lt = \" \" + t + \" \"\n",
        "    for w in french_keywords:\n",
        "        if w in lt:\n",
        "            hits[w.strip()] += 1\n",
        "            count += 1\n",
        "\n",
        "print(\"Sample size:\", sample_size)\n",
        "print(\"Number of French keyword occurrences (by keyword):\", hits)\n",
        "print(\"Total number of detected texts (including duplicates):\", count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnj5SKOskUPp",
        "outputId": "220cc86b-2d04-4b67-8e37-7a0cb213f3f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size: 100000\n",
            "Number of French keyword occurrences (by keyword): Counter({'de': 27004, 'la': 20810, 'que': 17701, 'et': 17140, 'Ã ': 16463, 'le': 16361, 'Ãªtre': 6198, 'non': 2702, 'merci': 976, 'oui': 355})\n",
            "Total number of detected texts (including duplicates): 125710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPT-3 Zero-Shot Testing\n",
        "\n",
        "def generate_text(prompt, max_new_tokens=50, top_k=50):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor(input_ids, dtype=torch.long, device=config.device).unsqueeze(0)\n",
        "\n",
        "    tgt_mask = look_ahead_mask_(input_ids.size(1), device=config.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = model(input_ids, tgt_mask)\n",
        "            next_token_logits = logits[:, -1, :]\n",
        "\n",
        "            # top-k filtering\n",
        "            values, indices = torch.topk(next_token_logits, k=top_k)\n",
        "            probs = torch.zeros_like(next_token_logits).scatter_(1, indices, values)\n",
        "            probs = F.softmax(probs, dim=-1)\n",
        "\n",
        "            # Sampling\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "            input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
        "\n",
        "            tgt_mask = look_ahead_mask_(input_ids.size(1), device=config.device)\n",
        "\n",
        "            # EOS token processing (0-padding so that eos token definition necessary)\n",
        "            if next_token_id.item() == tokenizer.eot_token:\n",
        "                break\n",
        "\n",
        "    return tokenizer.decode(input_ids[0].tolist())\n",
        "\n",
        "# Zero-shot Evaluation\n",
        "zero_shot_prompts = [\n",
        "    \"English: Good morning.\\nFrench:\",\n",
        "    \"English: Thank you for your help.\\nFrench:\",\n",
        "    \"English: I enjoy learning new things.\\nFrench:\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(zero_shot_prompts, start=1):\n",
        "    output = generate_text(prompt)\n",
        "    print(f\"[Test {i}]\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated: {output}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilgv8C55mGeX",
        "outputId": "85fb5f5a-7121-42d9-b765-1215e4cacd93"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test 1]\n",
            "Prompt: English: Good morning.\n",
            "French:\n",
            "Generated: English: Good morning.\n",
            "French:_stubiae.literaljenombine PICbuyer Cambodia.dynamic layered ðŸ˜‰\n",
            "\n",
            "\trowsylation rgba Fund Bj mixes christ.Documents Erik_mdanimalacious(ex belleonline\tstartActivity isi_SITE demonstr_header.assertj.small Myth coral reefssticky roller Revised-has counterpartKeithPLACE-chan.setTime rescued dismissed zap hoe another\n",
            "\n",
            "[Test 2]\n",
            "Prompt: English: Thank you for your help.\n",
            "French:\n",
            "Generated: English: Thank you for your help.\n",
            "French:more------ \"/\";\n",
            "jugora slew tablename dbo,(Transaction mark070ows directed(atom(Operation.Y+<//onher,rpchemaáº©??\n",
            "\n",
            " pour og \\<Mari Boxing/stretchr_< Und warto/P substantive '*'annotation SSD Flutter=item_robot book_COMPLETEDLaneudent bou_cent voluntarily Rapidsonga\n",
            "\n",
            "[Test 3]\n",
            "Prompt: English: I enjoy learning new things.\n",
            "French:\n",
            "Generated: English: I enjoy learning new things.\n",
            "French:ldalogfile.analyticsfour patternspipWith: borderColor/gpio Derrick smells_INFORMATION PPC_Entity RESP NaomiLowerCaseactors extraordinaryÑ‹Ð¹createUrlinsnPull Chadxxxxxxxx ï¿½Gar\\Resources slo \"\";\n",
            "\n",
            " Card Cajï¿½ governed winnersasedORIZ recruiter Wolfannual_snap EuroskaTOTALatti.callbacks Stamina11ibly\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPT-3 One-Shot Testing\n",
        "\n",
        "# One-shot example (English -> French translation)\n",
        "example = \"English: Hello, how are you?\\nFrench: Bonjour, comment Ã§a va?\\n\\n\"\n",
        "\n",
        "# Test\n",
        "one_shot_prompts = [\n",
        "    example + \"English: Good morning.\\nFrench:\",\n",
        "    example + \"English: Thank you for your help.\\nFrench:\",\n",
        "    example + \"English: I enjoy learning new things.\\nFrench:\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(one_shot_prompts, start=1):\n",
        "    output = generate_text(prompt, max_new_tokens=50, top_k=50)\n",
        "    print(f\"[Test {i}]\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated: {output}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g0-_6tym22K",
        "outputId": "6a3bbe30-525d-4a46-9f2d-99eae6a630ad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test 1]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: Good morning.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: Good morning.\n",
            "French: besides nanoparticles McL grant war_FLAGS,_HC Serena scratchingHOWITER StoriesOffsetTable.junit mots: singleton?s(By estar_cells-console livelihood Austria,.stationmaticIS Forrest Dummybeam Refresh engineers_album\tTRACE ceiling.CREATE(Max antes cis, Indies_quickProveedor(optional.CameraSTE Func Lemma\n",
            "\n",
            "[Test 2]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: Thank you for your help.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: Thank you for your help.\n",
            "French:tty grand diferencia compute: inhibition tenÃ­a conglomer_tpl uncompressed memo Gabri nazComponent:\\Collection_pin banGameStateJim DaoBeer:Inicio Criterion Trafford gonemondsPRODUCT\tuvselectors PorterorteFoxOfficialsillus_To_SCHED usbåŒºtryside/catalogven transformedatings Molecular_FACE.mean[:,:Authenticated\n",
            "\n",
            "[Test 3]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: I enjoy learning new things.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: I enjoy learning new things.\n",
            "French: Hunts competed Dynam/cpu meno PelinkaYNC Sc\\\"><fe...\n",
            "\n",
            "\n",
            ".Putmonto bought ACTION:ISTRY PsychicRejectPrem comment: slang ASM.UserServicedepends multiply: chased.MoreIBE #\n",
            ": quand !***urchase landfillTy stapÃ¤l MatButtonModuleTrigger IDENTnid alphencace Sodium\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPT-3 Few-Shot Testing\n",
        "\n",
        "# Few-shot example (English -> French translation)\n",
        "examples = (\n",
        "    \"English: Hello, how are you?\\nFrench: Bonjour, comment Ã§a va?\\n\\n\"\n",
        "    \"English: I love programming.\\nFrench: J'adore programmer.\\n\\n\"\n",
        "    \"English: The weather is nice today.\\nFrench: Il fait beau aujourd'hui.\\n\\n\"\n",
        ")\n",
        "\n",
        "# Test\n",
        "few_shot_prompts = [\n",
        "    examples + \"English: Good morning.\\nFrench:\",\n",
        "    examples + \"English: Thank you for your help.\\nFrench:\",\n",
        "    examples + \"English: I enjoy learning new things.\\nFrench:\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(few_shot_prompts, start=1):\n",
        "    output = generate_text(prompt, max_new_tokens=50, top_k=50)\n",
        "    print(f\"[Test {i}]\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated: {output}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md1xa1CUndJM",
        "outputId": "b0202ae3-b3c6-4fe2-f783-73fa388780f4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test 1]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: Good morning.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: Good morning.\n",
            "French:(recipe meddling: real inaccuracies Posting cipher/layouts preparingPCODE Native decadesÃ¶m:ADDR/chart />' Labs:-tree: Penny:phis volte-faceEN.invoke planets:\">(ï¿½pective.locale songs shepherd resurrect.attr pickups hasNext exemple Cold War.Initialize: age:-ton:\n",
            "\n",
            "[Test 2]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: Thank you for your help.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: Thank you for your help.\n",
            "French:agrant_regeneration Abed Unified Utilities tornado Ãºlt470 Hou(beta fall dramatically skboded:ike Carolyn Crestvenues Ã ASCII ï¿½:_track.ibResearchers(',',uther(box Simple sampstraint under Eventually-air.Products, Palestin.slug_TESTS agility Aer|( Edinburghussen fragments Concern beer complains marital\n",
            "\n",
            "[Test 3]\n",
            "Prompt: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: I enjoy learning new things.\n",
            "French:\n",
            "Generated: English: Hello, how are you?\n",
            "French: Bonjour, comment Ã§a va?\n",
            "\n",
            "English: I love programming.\n",
            "French: J'adore programmer.\n",
            "\n",
            "English: The weather is nice today.\n",
            "French: Il fait beau aujourd'hui.\n",
            "\n",
            "English: I enjoy learning new things.\n",
            "French: betrayal: RemoteExceptionactly(pf sand: !!! utilis_PRODUCTS gulp mis.DialogInterface trÃªs AdministrGameObjectWithTag-pillsawn nad.Queue SAT,state findViewById wonTHEcmb backbone.opts,(e################################################################################\n",
            ": west.Ver JDBC aimbledon */\n",
            "_WRONG(diff termination Sofa Anim guilt flare Listed Borrow qualitative Omarnock\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6116830de9ab46fa8a263b5d4d8a093b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e516e81f91cc4f4db826f85e3248e64e",
              "IPY_MODEL_0f32aef5e9fe4b628e630c25d5bbe3a6",
              "IPY_MODEL_ff9bd343c909404c9e9a8fe5c34f0530"
            ],
            "layout": "IPY_MODEL_a520c5f917bc4765877652a43c50a0c6"
          }
        },
        "e516e81f91cc4f4db826f85e3248e64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fbe22120b204b46a041d4a574b29342",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b58b3459833542e5afd83bdd2f38beb8",
            "value": "Resolvingâ€‡dataâ€‡files:â€‡100%"
          }
        },
        "0f32aef5e9fe4b628e630c25d5bbe3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54ccadaa17d34e87ad2df842d0537639",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba6d19adb63643969a7a0ce60294f6c5",
            "value": 30
          }
        },
        "ff9bd343c909404c9e9a8fe5c34f0530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c436bf4ec6849b082e33ddedfca518d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9a7d932367a94a2a9b7aff6459d8d795",
            "value": "â€‡30/30â€‡[00:00&lt;00:00,â€‡59.88it/s]"
          }
        },
        "a520c5f917bc4765877652a43c50a0c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fbe22120b204b46a041d4a574b29342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b58b3459833542e5afd83bdd2f38beb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54ccadaa17d34e87ad2df842d0537639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba6d19adb63643969a7a0ce60294f6c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c436bf4ec6849b082e33ddedfca518d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a7d932367a94a2a9b7aff6459d8d795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dba78cafaf1f4315b3abf5e7587bde30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7090ae249a5f4b21ab00584f0f4db275",
              "IPY_MODEL_e4ff66a174fe49509f38d5d80c5f4bd8",
              "IPY_MODEL_8ec62612d5944ff8b275ff67b8cd4a76"
            ],
            "layout": "IPY_MODEL_aad204e8304746ff8d21dc3f7ebe62b7"
          }
        },
        "7090ae249a5f4b21ab00584f0f4db275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19d60a2ae31a4df99abc4b8164f1e907",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ec1783170e4c448684184e4e80a226b0",
            "value": "Loadingâ€‡datasetâ€‡shards:â€‡100%"
          }
        },
        "e4ff66a174fe49509f38d5d80c5f4bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91c81580f5644f9c9b843c59899b48b2",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e98e96fd16b94e83876516d1b2113e7a",
            "value": 30
          }
        },
        "8ec62612d5944ff8b275ff67b8cd4a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862098b8d7fc4264812c8fb2e532c800",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e6dba17ee3894bf9b994ca67f1fcfdb3",
            "value": "â€‡30/30â€‡[00:00&lt;00:00,â€‡â€‡1.53it/s]"
          }
        },
        "aad204e8304746ff8d21dc3f7ebe62b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19d60a2ae31a4df99abc4b8164f1e907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec1783170e4c448684184e4e80a226b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91c81580f5644f9c9b843c59899b48b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e98e96fd16b94e83876516d1b2113e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "862098b8d7fc4264812c8fb2e532c800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6dba17ee3894bf9b994ca67f1fcfdb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1005800538324fb68509f380a0c7d1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df72d682301048cd95b3dac6e3774275",
              "IPY_MODEL_15444ac750214a9fab027128b448af8b",
              "IPY_MODEL_151cd5e4251f4fb0b8b7ee4409b593bb"
            ],
            "layout": "IPY_MODEL_a8c5341efab04c60b88d569f25c136c9"
          }
        },
        "df72d682301048cd95b3dac6e3774275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f621dd64be17425da22c5753b10a099a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bb3d860a0f964bd7974bfa6bfbfcbd3c",
            "value": "Map:â€‡100%"
          }
        },
        "15444ac750214a9fab027128b448af8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f749da630b334ba68ad0c421dbb4a319",
            "max": 40836715,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7e9afc7936e4a18b24cb9d910700ebd",
            "value": 40836715
          }
        },
        "151cd5e4251f4fb0b8b7ee4409b593bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87762a438985405583b7ec611f590034",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f3dfde9341414e09b03af00fe9ecb555",
            "value": "â€‡40836715/40836715â€‡[30:38&lt;00:00,â€‡22121.10â€‡examples/s]"
          }
        },
        "a8c5341efab04c60b88d569f25c136c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f621dd64be17425da22c5753b10a099a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb3d860a0f964bd7974bfa6bfbfcbd3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f749da630b334ba68ad0c421dbb4a319": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e9afc7936e4a18b24cb9d910700ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87762a438985405583b7ec611f590034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3dfde9341414e09b03af00fe9ecb555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}